---
title: "myApp-Dashboard"
author: "Edward Tandia"
date: "2023-11-23"
output:
 word_document: default
warning: no
always_allow_html: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Music analysis

This section is entirely built with R code. I will firstly reveal the EDA, then the different features implemented in the dashbaord mentioned in the methodology earlier. As a reminder, I am analysing a music dataset with metadata that have been arbitrarely selected (Artist, Genre, Lyrics, Track).

### Explanatory Data Analyis

Before exploring the data. A quick tokenization is neeeded to be able to work with the words.

```{r echo = F, cache = T, results = 'hide'}
pckg <- c('igraph','dplyr','readr','quanteda','quanteda.textstats',
          'lexicon','reshape2','ggplot2','text2vec','plotly','networkD3',
          'text','textdata','tidytext','utils', 'quanteda.textplots',
          'broom','ggwordcloud', 'plotly','png')

lapply(pckg,require,character.only =T)
## read file 
music_df <- read.csv('R_extra/music.csv') %>%
  data.frame() %>%
  select(
    Artist = artist_name,
    Genre = genre,
    Track = track_name,
    Lyrics = lyrics
  ) 


#tokenizations 

music_cp <- corpus(music_df$Lyrics)
music.tk <- tokens(
  music_cp,
  remove_numbers = TRUE,
  remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_separators = TRUE
) %>% 
  tokens_tolower() %>% 
  tokens_remove(stop_words$word) #%>% tokens_remove("Lyrics")
```

It is now possible to obtain a dfm (document frequency matrix), a tfidf matrix and a frequency list.

```{r echo = T }
## Compute the DTM, TF-IDF and global frequencies
music.dfm <- dfm(music.tk)
head(music.dfm)
```

The document frequency matrix point out the frequency of each term in each document.

```{r echo = T }
music.tfidf <-dfm_tfidf(music.dfm)  
head(music.tfidf)
```

The tfidf matrix shows the tfidf score of each terms within every document. The higher the score, the more specific the term.

```{r echo = T }
music.freq <- textstat_frequency(music.dfm)
head(music.freq)
```

The frequency list gives the frequency and the rank of each term. The lower the rank the more common the term. I notice that that 'time', 'feel', and 'yeah' are the most common words.

## Wordcloud

By using music.dfm, I generate a wordcloud where the terms "time" seems to be the prominent word. It also seems that the terms "feel", "life", "yeah", or "heart" may be quite frequent within the corpus.

```{r pressure, echo=FALSE, warning = F}

textplot_wordcloud(music.dfm)
```

## tf-idf PLOT

```{r echo = T, warning = F}


music.freq %>%
  top_n(10, frequency) %>%
plot_ly(
  x =  ~frequency,
  y =~reorder(feature, frequency),
  type = "bar"
  #orientation = "h"
) %>%
  layout(
    xaxis = list(title = "Frequency"),
    yaxis = list(title = "Terms"),
    title = 'Top 10 words frequency',
    margin = list(t = 100)
  )

```

I can notice the overwehlmed presence of " time " with more than 25k appereances in the corpus. Then come the terms 'feel', 'yeah' and 'heart'. All 3 values are close to each other with roughly 16k frequencies. The last term "world" has more than 10k frequencies which gives a good idea about the size of the corpus.

## Top 5 terms per document

```{r echo = T , warning= F}

set.seed(1234)
shuffled_indices <- sample(nrow(music.dfm))
shuffled_doc_matrix <- music.dfm[shuffled_indices, , drop = FALSE]
shuffled_doc_matrix[1:6,] %>% 
  tidy() %>% 
  group_by(document) %>% 
  top_n(5, count) %>% 
  ungroup() %>% 
  ggplot(aes(x = term, y = count)) + 
  geom_col() + coord_flip() + 
  theme(axis.text.y = element_text(size = 7),
        axis.ticks.y = element_blank())  + facet_wrap(~document, ncol = 2, scales="free")

```

6 documents have been randomly selected. Each document shows the count value of its the top 5 terms. Please note that in some documents, for example "text7656" there are more than 5 terms. Indeed some words have the same frequencies (in which case R keeps them all). Moreover, I observe that every shape is different. Based on this plot, I am assuming some signs of richness and variety describing the vocabulary of the given corpus.

## Top global 5 terms count in the given documents

```{r echo = T, warning = F}
set.seed(1234)
shuffled_indices <- sample(nrow(music.dfm))
shuffled_doc_matrix <- music.dfm[shuffled_indices, , drop = FALSE]
shuffled_doc_matrix[1:6,] %>% 
  tidy() %>% 
  top_n(20, count) %>% 
  ggplot(aes(x = term, y = count)) + 
  geom_bar(stat = "Identity") + 
  coord_flip() +
  theme(axis.text.y = element_text(size =7),
        axis.ticks.y = element_blank())  + 
  facet_wrap(~document, ncol = 2) 

```

In the given plot, it is possible to observe the frequency of the top 20 terms of the corpus in 6 documents randomly picked. For example, I notice that in "text24423" the term " romance " appears more more than 15 times. Here as well, there are only different shapes which can possibly indicates signs of a diverse vocabulary in the corpus.

## Top 10 max tf-idf terms in the corpus

```{r echo = T, warning= F}
music.tfidf %>% 
  tidy() %>%
  group_by(term) %>%
  summarize(count = max(count)) %>%
  ungroup() %>% 
  arrange(desc(count)) %>%
  top_n(10, count) %>%
  ggplot(aes(x=reorder(term, count),
             y = count)) + 
  geom_bar(stat = "identity") + 
  coord_flip() +
  xlab("Max TF-IDF") + 
  ylab("term") + 
  ggtitle('Top 10 max tf-idf terms in the corpus')

```

The plot illustrate a ranking of the max tf-idf of the most specific (rare) terms within the corpus. As a reminder, the higher the value , the rarer the term in the corpus. As an example, "bambiddlebam" has the highest tf-idf with a score of 347 which means it the most specific term in the corpus.

```{r echo=T , warning = F}
set.seed(1234)
music.freq %>%   
  ggplot(aes(x = log10(docfreq),
             y = log10(frequency))) + 
  geom_text(aes(label=feature),
            position=position_jitter(),
            size = 3.5) + 
  xlab("Document log-frequency") + 
  ylab("log-frequency")
```

Considering the high volume of terms I can only discuss the ones at the top right. This plot shows the log frequency of the terms. Once again, "time" has the highest value.Meaning is the most common term. Close to this word we can read 'feel' and 'yeah'.The graph is aligned with the wordcloud presented earlier.

## TTR

```{r echo = T, warning = F, timout = 1000}
set.seed(1234)
sample.dfm <- music.dfm[sample(nrow(music.dfm), 100), ] ## sample of size 100
sample.dfm%>% textstat_lexdiv()%>% 
  plot_ly(
    x = ~reorder(document, -TTR),
    y = ~TTR,
    type = 'bar'
  ) %>% 
  layout(
    xaxis = list(title = "Documents"),
    yaxis = list(title = 'TTR'))
```

When I try to plot all documents, I get a curve that it's bearely readable. However, I notice that there is a proportion of documents that has a score of 1 meaning some documents are have a rich vocabulary. At the opposite, there is also a proportion that has a very poor vocabulary with a score close or equal to 0. The x-axis is misleading it's hard to see exactly which document has a specific value. For instance if I try to hover on "text4907", it has a TTR OF 1 which means it is very diverse.

## MATTR MARCHE Pas

```{r}
#music.tk %>% 
 # textstat_lexdiv(
  #  measure = "MATTR",
   # MATTR_window = ) %>% 
  #ggplot(aes(reorder(
  #  document, -MATTR),
   # MATTR)) + 
  #geom_bar(stat="identity") +
  #xlab("Text")
```

# Keyness

```{r echo = T, warning = F}
music.keyness <- textstat_keyness(
    music.dfm,
    target = "text1002") # arbitrary choice
  textplot_keyness(music.keyness, labelsize = 3)

```

Here I picked 'text 1002' to be compared with a reference. I can notice a quite important difference between the 2. Reference is close to 0 and on text1002, there are different scores, especially "glow" = 106.91 chi2 score. Reference are the most common words within the corpus. According to the results, none of the term reference are in 'text 1002'. (pas sûr) c un test independence (statistique)

#A me dans discussion # The given shape is due to the diversity of the corpus. There are a lot of documents \# and with high vocabulary. (VOIR THEORIE)

```{r echo = T , warning = F}
  ## igraph --> REVOIR changer les dimensions

set.seed(123)
music.co <- fcm(music.tk[1:5,], 
                context = "document", 
                tri = FALSE)

music.co[music.co <= 17] <- 0
music.co[music.co > 17] <- 1
network <- graph_from_adjacency_matrix(
  music.co,
  mode = "undirected",
  diag = FALSE)
isolated <-which(degree(network) == 0)
G2 = delete.vertices(network, isolated)
#plot(network, layout = layout_with_kk )
plot(G2, layout = layout_with_kk )
```

Each co-occurrence is represented by an edge in the graph, but the abundance of edges can hinder readability. To address this issue, I've established a threshold: if the co-occurrence count is less than 17 (arbitrarily chosen), no link is created. However, for counts exceeding 17, a single link is established. Notably, 'feel' and 'till' emerge as central terms, exhibiting frequent co-occurrences with other terms. On the other hand, the terms 'time' and 'hold' can possibly appear in both clusters.

```{r echo = T, warning = F}
## X -ray plot 
  
textplot_xray(
  kwic(music.tk[1:100,], pattern = "time"),
  kwic(music.tk[1:100,], pattern = "feel"))
```

The 2 most frequent terms are displayed. And I can see different locations, of the terms within each given document. For example, in document 'text1', 'time' appears twice, at the beginning and at the end. 'feel' appears twice at \~20% of the 'text1' lyrics, twice at \~25%, twice at 60% , twice at \~80% and once at the end \~98%.

\# Revoir la thérorie

# Sentiment analysis

Sentiment analysis is where I show how predefined sentiments interact with a track, an album, an artist or a music genre can generate.

```{r echo= FALSE}
shuffled_df <- music_df[sample(nrow(music_df)), ]

# Tokenization
music.tb <- as_tibble(shuffled_df)
music.tok <- tidytext::unnest_tokens(
  music.tb,
  output = "word",
  input = "Lyrics",
  to_lower = TRUE,
  strip_punct = TRUE,
  strip_numeric = TRUE
)


music.sent <- inner_join(
  x = music.tok,
  y = tidytext::get_sentiments('nrc'), #textdata::lexicon_nrc(), 
  by = c("word" = "word"),
  relationship = 'many-to-many'
)
```

```{r echo = T, warning = F}
getGenreMood <- function (genre){ # assuming it's "pop"     "country" "blues"   "jazz"    "reggae"  "rock"    "hip hop"    
  
  if (genre==''){
    stop('No genre submitted. Please submit one.')
  }else{ 
    return (music.sent[music.sent$Genre == genre,] %>% 
              group_by(Genre, sentiment) %>% 
              summarize(n = n()) %>%
              ungroup() %>% 
              plot_ly(x = ~n, 
                      y = ~sentiment,
                      color = ~sentiment,
                      type = 'bar',
                      mode = 'markers') %>% 
              layout(title = genre, margin = list(t = 30)))
  }
}
getGenreMood('jazz')
```

In this example, music genre illustrates the quantity of how sentiments such as anger,anticipation, disgust, fear, joy, negative, positive, sadness surprise and trust impact the genre jazz. According to he plot, jazz seems to generate high positive emotions, however it might produce negative feelings as well.

```{r echo = T, warning= F}
getTraksentiement <- function(track){
  
  if (track == ''){
    stop('No track submitted. Please insert one.')
  }else{
    
    ###  same idea for each songs
    #music.sent_shuffuled <- music.sent[sample(nrow(music.sent)),] 
    
    return (music.sent[music.sent$Track == track,] %>% # filter and keep only track given
              group_by(Track, sentiment) %>% 
              summarize(n = n()) %>%
              ungroup() %>% 
              plot_ly(x = ~n, 
                      y = ~sentiment,
                      color = ~sentiment,
                      type = 'bar',
                      mode = 'markers') %>% 
              layout(title = track, margin = list(t = 60)))
  }
}
getTraksentiement('over the rainbow')
```

I picked the famous song 'over the rainbow' by IZ. It seems that negative emotions are dominant. Positive emotions are quite close. In addition, sentiment such as sadness and trust are also noticeable.

```{r echo = T, warning = F }
getArtistSentiement <- function(artist){
  
  if (artist == ''){
    print('No artist submitted. Please submit one.')
    stop()
  }else{
    
    ###  same idea for each songs
    #music.sent_shuffuled <- music.sent[sample(nrow(music.sent)),] 
    
    return (music.sent[music.sent$Artist == artist,] %>% # filter and keep only track given
              group_by(Artist, sentiment) %>% 
              summarize(n = n()) %>%
              ungroup() %>% 
              plot_ly(x = ~n, 
                      y = ~sentiment,
                      color = ~sentiment,
                      type = 'bar',
                      mode = 'markers') %>% 
              layout(title = artist, margin = list(t = 60)))
    
  }
  
}

getArtistSentiement('l7')
```

Here, the rock band 'L7' is picked. A strong and overwehlming negative feeling stands out. Meaning, listening to L7 may give negative emotions.

# GloVe model

For faster computations, I decided to work with a sample of the data set called "shuffled_music". It only contains 100 rows. The app is also functional with this sample.

The GloVe model is arbitrarly defined with a window = 10 and weights = 1/( sequence 1 to 10 ). I use the frequency matrix to build a coocurrence matrix then I fit the glove model to it with 100 epochs (iterations). This will learn the word vectors (word_vectors_main) for the main words in the corpus. In addition, I extract the context word vectors from the trained GloVe model.The final step is to combine the main word vectors with the transposed context word vectors.

```{r echo = F, cache= T}

set.seed(1234)# reproductibility 

shuffuled_music <- music_df[sample(nrow(music_df)),] 
shuffuled_music3 <- shuffuled_music[1:100,]
crude.cp <- corpus(shuffuled_music3$Lyrics)

crude.tk <- tokens(
  crude.cp,
  remove_numbers = TRUE,
  remove_punct = TRUE,
  remove_symbols = TRUE, 
  remove_separators=TRUE)%>% 
  tokens_tolower() %>% 
  tokens_remove(stop_words$word) %>% tokens_replace(.,
  pattern = hash_lemmas$token,
  replacement = hash_lemmas$lemma)

```

```{r echo = T, warning= F, results= 'hide'}
fcm <- fcm(crude.tk,  ## co-occurence matrix
           context = "window",
           count = "weighted",
           window= 10, # mean result of lyrics set
           weights = 1/(1:10), # custom 
           tri = FALSE)

glove <- GlobalVectors$new(rank = 2, x_max = 10) # model 

word_vectors_main <- glove$fit_transform(fcm, n_iter = 100) # a changer !
word_vectors_context <- glove$components
model.glove <- word_vectors_main + t(word_vectors_context)

```

```{r echo =T, warning = F}
# plot 
set.seed(123)
index <- textstat_frequency(dfm(crude.tk))[1:50, ]$feature
data.for.plot <- data.frame(word_vectors_main[index, ])
data.for.plot$word <- row.names(data.for.plot)
ggplot(data.for.plot, 
       aes(x = X1,
           y = X2,
           label = word)) +
  ggrepel::geom_text_repel(max.overlaps = 100) + 
  theme_void() 

```

This plot is an example of words embedding. Words that are close to each other are often used in the similar context. For example, the term 'disappear' may appear with the word 'home' or 'ready'.

```{r echo =F, warning = F}


## RWMD algo
dtm<-dfm(crude.tk)
speech.rwmd.mode<-RelaxedWordMoversDistance$new(dtm,model.glove)
speech.rwms <- speech.rwmd.mode$sim2(dtm)
speech.rwmd <- speech.rwmd.mode$dist2(dtm)

#######


#myfunct2 <- function(){
mylist <- c()
mylist_w<-c()
l <- shuffuled_music3$Artist
w <-  shuffuled_music3$Track

for (i in 1:length(l)){
  mylist<-append(mylist,l[i])
}
for(i in 1:length(w)){
  mylist_w<-append(mylist_w,w[i])
}

mylist <- mylist
mylist_w <-mylist_w
  
#} # get name lists
#myfunct2()


row.names(speech.rwmd) <- mylist#crude.tk2
colnames(speech.rwmd)<-mylist

X1 <- c('anvil','l7','suicidal tendencies','freddy fender','bobby darin',
        'the kills','zz top','roger miller','metallica','sudan archives','j-kwon',
        'gerry rafferty','chrus ledbux','the original squad','ben folds',
        'kelly clarkson','the kills', 'ty segall','zz top','the afters',
        'bush','paul simon', 'the kills','paul simon', 'metric','welshly arms',
        'three days grace','donna summer', 'a day to remember','steve oliver',
        'phish','kelly clarkson', 'ty segall','rem')




X2 <-c('phish','phish','phish','phish','phish','phish',
       'marty robbins','marty robbins','marty robbins','marty robbins','marty robbins',
       'marty robbins','marty robbins','marty robbins','marty robbins','marty robbins',
       'marty robbins','marty robbins','blind willie johnson','blind willie johnson',
       'blind willie johnson','blind willie johnson','blind willie johnson',
       'bobby darin','bobby darin','bobby darin','bobby darin','bobby darin',
       'bobby darin','bobby darin','bobby darin','bobby darin','bobby darin',
       'three days grace') 

networkData <- data.frame(X2,X1)
```

# Artist recommendations

```{r echo = T, eval = F}

# text recommendations
set.seed(123)
#recoFunction <- function(artist_name){
mytext<-speech.rwmd[,'peter tosh'] %>% sort(.,decreasing = T) %>% head() %>% names()

for (num in seq_along(mytext)) {
  current_result <- paste(num, ':', mytext[num])
  print(current_result)
 # res <- c(res, current_result)
}
 #  output retuerned 

#"1 : r.l. burnside"
#"2 : ricky nelson"
#"3 : waylon jennings"
#"4 : linton kwesi johnson"
#"5 : freddy fender"
#"6 : ricky nelson"

#  return(res)
#}

#recoFunction('')
```

# Network

```{r echo = T, warning = F}

simpleNetwork(networkData, height = 1000, width = 1000,opacity = 0.7,
                                nodeColour = '#FF00FF',zoom = T)

```

# Dashboard features

Let's explore the myAPP Dashboard features. Once every function are set in shiny, the user can interact with the app.

## Dashboard

![](images/Capture%20d’écran%202023-11-29%20à%2012.19.42.png)

This is the design of the final dashboard. It is divided in two parts : the inputs section on the left hand and the outputs on the right one.

## Artist Recommendation and network

![](images/artist_recommendation.png)

The first feature ' Artist Recommendations' suggest a dropdown list of artists. Once one is selected, the result is displayed on the blue box, as a rank. Value 1 has the highest similarity score and 6 the lowest. Below it, the user can also observe a network that is composed of every artist with a minimum similarity score of 0.85.

## Genre sentiment analysis

![](images/Capture%20d’écran%202023-11-29%20à%2012.26.43.png)

Genre music feature highlights the count of each given sentiment related to the genre. Here, it seems 'jazz' gives a strong positive feeling, but it can also generate negative emotions...

## Artist sentiment analysis

![](images/artist_analysis.png)

I selected the artist " ricky nelson " for the artist sentiment analysis feature. I can interpret that the artist brings positive feelings, especially joy. Though, he can also express negative sentiments.

## Track sentiment analysis

![](images/track_anaylsis.png)

The last feature is track sentiment analysis. I picked the track "sweeter than you" by Ricky Nelson which seems to bring high positive feelings such as joy.

# Merging

```{r echo = T, eval = F}
rsconnect::deployApp('shiny')
```

![](R_extra/logs.png)

Unfortunately the merging attempt failed. The logs give an approximate location of where the error lies. I notice the quote "Warning: Error in menu: menu() cannot be used non-interactively". Meaning the function menu() expects an interaction with the user which is not possible here.

# DISCUSSION

Discussion section is dedicated to comment the results in order to have a better understanding of how the given outputs have been obtained. Moreover this part is also relevant because I discuss the limitations of some incentives taken while suggesting order options that might be more adequate.

Music Recognition

First of all, Music recognition section meets the formulated hypothesis. The frontend matches what I stated and every buttons are operational. Indeed all 3 buttons stated are present. Music finder button redirects to music finder section, sign in and sign up buttons allow the user to create and/or log in his account, and finally my App button, once clicked, activates the device's microphone to capture the audio. Even though all features are functional, my App button function is dysfunctional. The issue may come from my own laptop microphone that cannot properly record the audio. I was not able to test the function on another device therefore, it is hard to identify the origin of the issue. Testing on another hardware should be done to clearly understand the problem. At this point, the detection tracks function either renders no results or random songs.

Music finder

Secondly, Music Finder delivers great outputs. The design is inspired from a template that has been picked arbitrarily. I edited some CSS code to make the whole code fit the outputs I defined earlier (images and input name).  In the dropdown list, a blank input has been added. I was not able to capture a pre-selected option, therefore I just defined a blank input to avoid unnecessary clicks. Genre music query is not functional because spotify api requires the ID of a track or an album or an artist to process the query. My idea was to generate a list of genres with the artist submitted accordingly but this is not one of the defined hypothesis. Thus I restricted the genre filter. Though the section meets the requirements there is still room for improvement. For instance, adding a function that could load a song when clicking on an artist photo, or the user could be redirected to the artist bio page and get more insights about him. This can be done with some javascript code and the use of the spotify API.

Music analysis

EDA PART

EDA is quite interesting. Based on the documents explored in " top 5 terms per document " and " top global 5 terms count in the given documents" , every document has its own shape which points out signs of variety and richness of the global vocabulary. In addition, the Max TF-IDF plot highlights high tf-idf scores, up to 347. There is also TTR that shows seral documents with a score of 1 indicating variety as well. These are two other arguments that orient my thought towards a variate vocabulary. However, I cannot guarantee that this statement is validated to the whole corpus since it has a huge size (more than 1M token). The lack of technical resources prevented me to explore the whole dataset. Thus this huge dataset seems to be diverse enough to experiment different hypothesis and produce interesting results. --\> à revoir avec théorie.

sentiment analysis

Secondly the results obtain in the sentiment analysis seem divergent. Since I picked a specific value for every feature I can only discuss the given cases. Each sentiment plot may or may not reflects the reality. Indeed music is a field where critism is subjective therefore I can only assume that the suggested emotions should be expected according to the plot. Results have different shapes. They have to be taken with a grain of salt. I might have had better results if I could analyse each track and maybe train a robust model based on these that could predict the expected emotions more efficiently. In overall, the dictionary used seem effective and it is capable to give the expected emotions accordingly.

GloVe model

Thanks to the GloVe model I could give birth to the Artist recommendations and a network features. Artist recommendations is able to recommend artists based on a given value. According to the GloVe model alogirthm, the artist recommendation feature should have considered the context of the given lyrics in order to produce a ranking of artists suggestion. In addition, the model has also been helpful to generate the graph. Unfortunately, I can only provided a network based on a 100 documents sample. The graph can gain more complexity if the size of the sample is increased. This demands a more powerful computing tool. Therefore, I restricted myself to the code implemented and its logic. In order to be sure of the robustness of the outputs, a robust model capable of verifying the reliability of every output would be wise.

Dashboard part :

In overall the dashboard delivers the requirements. It is user-friendly, and the user can easily interact with the different options. However I had to narrow the dataset down to a sample of 100 values. The app would have crashed, or taken way too much time to displays the outputs. Therefore every features are built with a dropdown list in order to make the app run smoothly. As mentioned earlier, a more powerful hardware might perform better and deal with more input data.

Merging

The merge is the final step of the whole. Indeed conciliate music recognition , music finder and dashboard sections altogether. Unfortunately I did not manage to achieve this goal. While reading the logs, I keep encountering the path " ... " (add path from log) does not exist. My thought is that the predifined path within the lexicon_nrc() function needs to be edited however I did not find a way to do so. Consequently to access the dashboard section in myApp the user must have Rstudio installed alongside the required packages.

To sum up, the discussion section critically evaluates the study's outcomes, highlighting limitations and proposing improvements. In the Music Recognition segment, while the frontend aligns with the hypothesis, functionality issues arise with the "My App" button, potentially linked to the laptop's microphone. The Music Finder section successfully delivers outputs, but a non-functional genre query and suggestions for additional features are noted. The Music Analysis, particularly the Exploratory Data Analysis, showcases a diverse vocabulary in documents, though technical constraints limit exploration of the entire dataset. Sentiment Analysis results show divergence, acknowledging the subjective nature of music criticism. The GloVe model contributes Artist Recommendations and network features, but limitations in sample size and computing power are recognized. The Dashboard meets requirements but is restricted to a sample of 100 values for smoother performance, while challenges persist in merging sections due to path-related issues. Overall, the study underscores achievements, acknowledges constraints, and suggests avenues for enhancement across various analytical and application facets.

**Interpretation of Results:**

1.  

    -   Provide detailed interpretations of your results.

    -   Discuss the significance and relevance of each finding.

    -   Compare and contrast your results with existing literature.

2.  **Relation to Research Questions/Hypotheses:**

    -   Discuss how your results align with or deviate from your initial research questions or hypotheses.

    -   Explore any unexpected outcomes and potential reasons behind them.

3.  **Comparison with Previous Studies:**

    -   Compare your results with findings from other studies in the field.

    -   Highlight similarities, differences, and potential reasons for variations.

4.  **Limitations:**

    -   Acknowledge any limitations in your study.

    -   Discuss how these limitations might have influenced the results.

    -   Suggest areas for future research that could address these limitations.

5.  **Implications:**

    -   Discuss the broader implications of your findings.

    -   Consider the practical, theoretical, or policy implications.

    -   Discuss how your results contribute to the existing body of knowledge.

6.  **Recommendations:**

    -   Provide recommendations based on your findings.

    -   Discuss practical steps or strategies that could be implemented.

    -   Consider any changes to methodologies for future studies.

7.  **Conclusion:**

    -   Summarize the main points discussed in the section.

    -   Reinforce the significance of your findings and their contribution to the field.

# CONCLUSION

This is the final part of the project. It has been stated that the purpose of this work consists of developing a digital application from scratch with the help of different languages. In order to achieve this work, I first had to get some knowledge about HTML, CSS and javascript. Once the basics acquired, I built the two sections music recognition and music finder. Both are well designed however regarding the former, the music recognition function does not seem to be able to capture a song properly. Music finder works well and its design is also dynamic which emphasizes the interaction with the user. The sections deliver all query with an image and the name of every outputs. Unfortunately the algoithm does not provide a response when the filter genre music is submitted. Shop33ify api is configured differently for this query. Finally the music analysis section. The layout meets the formal hypothesis, an interactive dashboard with the inputs query defined in some dropdown lists and the output visualizations on the right side. The dashboard is user-friendly and allows the user to see the different emotions a track, an artist, or a music genre can produce to him according to the input. Moreover, the dashboard can also recommend 6 artists that are close to the artist submitted. It is also possible to visualize a full network composed of every artists with a high similarity score. Finally the EDA has shown important signs of a diversity and richness of the corpus. Indeed different shapes of the analyzed documents when counting terms, the variate tf-idf scores from 0 to 347 or the right-queued distribution of TTR are all elements that orient my thoughts to diversity. This leaves room for different outcomes depending on the work undertaken with the data set.

In addition, although every section were built, several limitations have been encountered. Firstly, while designing the frontend of both music finder and music recognition sections, I faced some difficulties in creating a server to store users data and session. The lack of coding skills is mainly the reasons why I can only provide data storage but no user session. This failed attempt prevented me to attribute each user's activity and tracking. So far it is only possible to gather the data in general without any user filter. Regardless the user, all the data is stored in different sections (albums, genres,artists, and tracks). The idea was to provide a user tracking activity over time in music analysis section afterwards. Secondly, as mentioned before, the main button function my app can barely catch an audio sample. The origin of the problem is not identified. Trying the function on another device might be the solution. Otherwise other debugging attempts are necessary. Regarding the dataset used in the shiny dashboard, the app can only work with a sample and not the whole dataset. Tough the section provides different functions, the computations are set for faster time request and the dataset implemented has been reduced to a sample of 100 values. The lack of technical resources forced me to reduce the variety of possibilities. With more sophisticated technical tools the outcomes can be more diversed and interesting. Last but not least, the merging phase attempt. The deployment on the internet with Shinyapps.io does not work. A path issue that somehow I am not able to fixed interrupts the entire processing. Therefore the app is seperated in two entities instead of one. Any one who wants to use it must have Rstudio to run shiny.

To conclude, I am trhilled to say that this project has been an exciting journey. I have gained more experience in coding in javascript, HTML, and CSS, I also could stregnthened my skills in R. Data management on MongoDB is also a field that I discovered. Both frontend and backend have become more familiar. Indeed being able to create an interactive interface, with dynamics visualizations and responses enhance the user experience. Moreover, developing the backend had not only made me realize how dependable both sides are (client side, server side) but it also brought me a deeper and clearer understanding of the connections between these two. Finally this work is an example of how an app with a main function song recognition can easily be improved by adding other relevent sections without deep code restructurations or constructions. This demonstrates that an idea can be rethought to something greater. In the project scopre, more features can be implemented. The possibilities are endless.

As a final word, I would like to thank professor Marcel Baumgartner for having being open to the elaboration of this project and his support, including advice, tool suggestions and the report conception during the semester.

data storage (sessoin , token, server reponse rending, ) music recognition, dataset resctrtion, merging.

limitations:

objectives (different languages, same layout as describe, 3 sections,)

1.  **Summary of Key Findings:**

    -   Provide a concise recap of the main findings presented in the report.

2.  **Achievement of Objectives/Research Questions:**

    -   Discuss how well your study addressed the initial objectives or research questions.

3.  **Contributions to Knowledge:**

    -   Highlight the novel contributions your study makes to the existing body of knowledge in the field.

4.  **Implications for Practice/Policy:**

    -   Summarize the practical or policy implications of your findings.

    -   Discuss how your results can be applied in real-world scenarios.

5.  **Theoretical Implications:**

    -   Discuss any theoretical contributions or advancements resulting from your study.

6.  **Limitations Recap:**

    -   Revisit and summarize the limitations discussed in the report.

7.  **Recommendations for Future Research:**

    -   Provide specific recommendations for future research based on the gaps identified in your study.

8.  **Overall Conclusion:**

    -   Offer a general conclusion that ties together the main points discussed in the report.

    -   Emphasize the overall significance of your study.

9.  **Closing Remarks:**

    -   Include any final thoughts or remarks.

    -   Consider the broader implications of your work for the field or related disciplines.

10. **Acknowledgment of Participants/Contributors:**

-   If applicable, acknowledge any individuals or organizations that contributed to the study.
